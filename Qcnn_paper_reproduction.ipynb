{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quaternion-Neural-Networks\n",
        "\n",
        "## Group 43\n",
        "\n",
        "\n",
        "source ([2019 code](https://github.com/heblol/Pytorch-Quaternion-Neural-Networks/blob/master/core_qnn/quaternion_layers.py)) - pytorch  \n",
        "\n",
        "source ([2018 code](https://github.com/XYZ387/QuaternionCNN_Keras/blob/master/cifar10_cnn.py)) - tensorflow\n",
        "\n",
        "<!-- @inproceedings{\n",
        "parcollet2018quaternion,\n",
        "title={Quaternion Recurrent Neural Networks},\n",
        "author={Titouan Parcollet and Mirco Ravanelli and Mohamed Morchid and Georges Linarès and Chiheb Trabelsi and Renato De Mori and Yoshua Bengio},\n",
        "booktitle={International Conference on Learning Representations},\n",
        "year={2019},\n",
        "url={https://openreview.net/forum?id=ByMHvs0cFQ},\n",
        "} -->\n",
        "\n"
      ],
      "metadata": {
        "id": "THrCEae3Acau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################\n",
        "# pytorch-qnn v1.0\n",
        "# Titouan Parcollet\n",
        "# LIA, Université d'Avignon et des Pays du Vaucluse\n",
        "# ORKIS, Aix-en-provence\n",
        "# October 2018\n",
        "##########################################################\n",
        "\n",
        "import numpy                   as np\n",
        "from   numpy.random            import RandomState\n",
        "import torch\n",
        "from   torch.autograd           import Variable\n",
        "import torch.nn.functional      as F\n",
        "import torch.nn                 as nn\n",
        "from   torch.nn.parameter       import Parameter\n",
        "from   torch.nn                 import Module\n",
        "\n",
        "\n",
        "import math\n",
        "import sys\n",
        "\n",
        "# Imports for the quaternion_ops\n",
        "import torch.nn as nn\n",
        "from scipy.stats import chi\n",
        "import pdb"
      ],
      "metadata": {
        "id": "iHg7-Z-9AUqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import quaternion ops\n",
        "core_qnn/quaternion_ops.py\n",
        "\n",
        "I think these are helper functions."
      ],
      "metadata": {
        "id": "qTPSKwNrfsZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_normalize(input, channel=1):\n",
        "\n",
        "    r = get_r(input)\n",
        "    i = get_i(input)\n",
        "    j = get_j(input)\n",
        "    k = get_k(input)\n",
        "\n",
        "    norm = torch.sqrt(r*r + i*i + j*j + k*k + 0.0001)\n",
        "    r = r / norm\n",
        "    i = i / norm\n",
        "    j = j / norm\n",
        "    k = k / norm\n",
        "\n",
        "    return torch.cat([r,i,j,k], dim=channel)\n",
        "\n",
        "\n",
        "def check_input(input):\n",
        "\n",
        "    if input.dim() not in {2, 3, 4, 5}:\n",
        "        raise RuntimeError(\n",
        "            \"Quaternion linear accepts only input of dimension 2 or 3. Quaternion conv accepts up to 5 dim \"\n",
        "            \" input.dim = \" + str(input.dim())\n",
        "        )\n",
        "\n",
        "    if input.dim() < 4:\n",
        "        nb_hidden = input.size()[-1]\n",
        "    else:\n",
        "        nb_hidden = input.size()[1]\n",
        "\n",
        "    if nb_hidden % 4 != 0:\n",
        "        raise RuntimeError(\n",
        "            \"Quaternion Tensors must be divisible by 4.\"\n",
        "            \" input.size()[1] = \" + str(nb_hidden)\n",
        "        )\n",
        "#\n",
        "# Getters\n",
        "#\n",
        "def get_r(input):\n",
        "    check_input(input)\n",
        "    if input.dim() < 4:\n",
        "        nb_hidden = input.size()[-1]\n",
        "    else:\n",
        "        nb_hidden = input.size()[1]\n",
        "\n",
        "    if input.dim() == 2:\n",
        "        return input.narrow(1, 0, nb_hidden // 4)\n",
        "    if input.dim() == 3:\n",
        "        return input.narrow(2, 0, nb_hidden // 4)\n",
        "    if input.dim() >= 4:\n",
        "        return input.narrow(1, 0, nb_hidden // 4)\n",
        "\n",
        "\n",
        "def get_i(input):\n",
        "    if input.dim() < 4:\n",
        "        nb_hidden = input.size()[-1]\n",
        "    else:\n",
        "        nb_hidden = input.size()[1]\n",
        "    if input.dim() == 2:\n",
        "        return input.narrow(1, nb_hidden // 4, nb_hidden // 4)\n",
        "    if input.dim() == 3:\n",
        "        return input.narrow(2, nb_hidden // 4, nb_hidden // 4)\n",
        "    if input.dim() >= 4:\n",
        "        return input.narrow(1, nb_hidden // 4, nb_hidden // 4)\n",
        "\n",
        "def get_j(input):\n",
        "    check_input(input)\n",
        "    if input.dim() < 4:\n",
        "        nb_hidden = input.size()[-1]\n",
        "    else:\n",
        "        nb_hidden = input.size()[1]\n",
        "    if input.dim() == 2:\n",
        "        return input.narrow(1, nb_hidden // 2, nb_hidden // 4)\n",
        "    if input.dim() == 3:\n",
        "        return input.narrow(2, nb_hidden // 2, nb_hidden // 4)\n",
        "    if input.dim() >= 4:\n",
        "        return input.narrow(1, nb_hidden // 2, nb_hidden // 4)\n",
        "\n",
        "def get_k(input):\n",
        "    check_input(input)\n",
        "    if input.dim() < 4:\n",
        "        nb_hidden = input.size()[-1]\n",
        "    else:\n",
        "        nb_hidden = input.size()[1]\n",
        "    if input.dim() == 2:\n",
        "        return input.narrow(1, nb_hidden - nb_hidden // 4, nb_hidden // 4)\n",
        "    if input.dim() == 3:\n",
        "        return input.narrow(2, nb_hidden - nb_hidden // 4, nb_hidden // 4)\n",
        "    if input.dim() >= 4:\n",
        "        return input.narrow(1, nb_hidden - nb_hidden // 4, nb_hidden // 4)\n",
        "\n",
        "\n",
        "def get_modulus(input, vector_form=False):\n",
        "    check_input(input)\n",
        "    r = get_r(input)\n",
        "    i = get_i(input)\n",
        "    j = get_j(input)\n",
        "    k = get_k(input)\n",
        "    if vector_form:\n",
        "        return torch.sqrt(r * r + i * i + j * j + k * k)\n",
        "    else:\n",
        "        return torch.sqrt((r * r + i * i + j * j + k * k).sum(dim=0))\n",
        "\n",
        "\n",
        "def get_normalized(input, eps=0.0001):\n",
        "    check_input(input)\n",
        "    data_modulus = get_modulus(input)\n",
        "    if input.dim() == 2:\n",
        "        data_modulus_repeated = data_modulus.repeat(1, 4)\n",
        "    elif input.dim() == 3:\n",
        "        data_modulus_repeated = data_modulus.repeat(1, 1, 4)\n",
        "    return input / (data_modulus_repeated.expand_as(input) + eps)\n",
        "\n",
        "\n",
        "def quaternion_exp(input):\n",
        "\n",
        "    r      = get_r(input)\n",
        "    i      = get_i(input)\n",
        "    j      = get_j(input)\n",
        "    k      = get_k(input)\n",
        "\n",
        "\n",
        "    norm_v = torch.sqrt(i*i+j*j+k*k) + 0.0001\n",
        "    exp    = torch.exp(r)\n",
        "\n",
        "    r      = torch.cos(norm_v)\n",
        "    i      = (i / norm_v) * torch.sin(norm_v)\n",
        "    j      = (j / norm_v) * torch.sin(norm_v)\n",
        "    k      = (k / norm_v) * torch.sin(norm_v)\n",
        "\n",
        "\n",
        "    return torch.cat([exp*r, exp*i, exp*j, exp*k], dim=1)\n",
        "\n",
        "\n",
        "def quaternion_conv(input, r_weight, i_weight, j_weight, k_weight, bias, stride,\n",
        "                    padding, groups, dilatation):\n",
        "    \"\"\"\n",
        "    Applies a quaternion convolution to the incoming data:\n",
        "    \"\"\"\n",
        "\n",
        "    cat_kernels_4_r = torch.cat([r_weight, -i_weight, -j_weight, -k_weight], dim=1)\n",
        "    cat_kernels_4_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=1)\n",
        "    cat_kernels_4_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=1)\n",
        "    cat_kernels_4_k = torch.cat([k_weight,  -j_weight, i_weight, r_weight], dim=1)\n",
        "\n",
        "    cat_kernels_4_quaternion   = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=0)\n",
        "\n",
        "    if   input.dim() == 3:\n",
        "        convfunc = F.conv1d\n",
        "    elif input.dim() == 4:\n",
        "        convfunc = F.conv2d\n",
        "    elif input.dim() == 5:\n",
        "        convfunc = F.conv3d\n",
        "    else:\n",
        "        raise Exception(\"The convolutional input is either 3, 4 or 5 dimensions.\"\n",
        "                        \" input.dim = \" + str(input.dim()))\n",
        "\n",
        "    return convfunc(input, cat_kernels_4_quaternion, bias, stride, padding, dilatation, groups)\n",
        "\n",
        "def quaternion_transpose_conv(input, r_weight, i_weight, j_weight, k_weight, bias, stride,\n",
        "                    padding, output_padding, groups, dilatation):\n",
        "    \"\"\"\n",
        "    Applies a quaternion trasposed convolution to the incoming data:\n",
        "    \"\"\"\n",
        "\n",
        "    cat_kernels_4_r = torch.cat([r_weight, -i_weight, -j_weight, -k_weight], dim=1)\n",
        "    cat_kernels_4_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=1)\n",
        "    cat_kernels_4_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=1)\n",
        "    cat_kernels_4_k = torch.cat([k_weight,  -j_weight, i_weight, r_weight], dim=1)\n",
        "    cat_kernels_4_quaternion   = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=0)\n",
        "\n",
        "\n",
        "    if   input.dim() == 3:\n",
        "        convfunc = F.conv_transpose1d\n",
        "    elif input.dim() == 4:\n",
        "        convfunc = F.conv_transpose2d\n",
        "    elif input.dim() == 5:\n",
        "        convfunc = F.conv_transpose3d\n",
        "    else:\n",
        "        raise Exception(\"The convolutional input is either 3, 4 or 5 dimensions.\"\n",
        "                        \" input.dim = \" + str(input.dim()))\n",
        "\n",
        "    return convfunc(input, cat_kernels_4_quaternion, bias, stride, padding, output_padding, groups, dilatation)\n",
        "\n",
        "\n",
        "def quaternion_conv_rotation(input, zero_kernel, r_weight, i_weight, j_weight, k_weight, bias, stride,\n",
        "                    padding, groups, dilatation, quaternion_format, scale=None):\n",
        "    \"\"\"\n",
        "    Applies a quaternion rotation and convolution transformation to the incoming data:\n",
        "    The rotation W*x*W^t can be replaced by R*x following:\n",
        "    https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n",
        "    Works for unitary and non unitary weights.\n",
        "    The initial size of the input must be a multiple of 3 if quaternion_format = False and\n",
        "    4 if quaternion_format = True.\n",
        "    \"\"\"\n",
        "\n",
        "    square_r          = (r_weight*r_weight)\n",
        "    square_i          = (i_weight*i_weight)\n",
        "    square_j          = (j_weight*j_weight)\n",
        "    square_k          = (k_weight*k_weight)\n",
        "\n",
        "    norm              = torch.sqrt(square_r+square_i+square_j+square_k + 0.0001)\n",
        "\n",
        "    #print(norm)\n",
        "\n",
        "    r_n_weight          = (r_weight / norm)\n",
        "    i_n_weight          = (i_weight / norm)\n",
        "    j_n_weight          = (j_weight / norm)\n",
        "    k_n_weight          = (k_weight / norm)\n",
        "\n",
        "    norm_factor       = 2.0\n",
        "\n",
        "    square_i          = norm_factor*(i_n_weight*i_n_weight)\n",
        "    square_j          = norm_factor*(j_n_weight*j_n_weight)\n",
        "    square_k          = norm_factor*(k_n_weight*k_n_weight)\n",
        "\n",
        "    ri                = (norm_factor*r_n_weight*i_n_weight)\n",
        "    rj                = (norm_factor*r_n_weight*j_n_weight)\n",
        "    rk                = (norm_factor*r_n_weight*k_n_weight)\n",
        "\n",
        "    ij                = (norm_factor*i_n_weight*j_n_weight)\n",
        "    ik                = (norm_factor*i_n_weight*k_n_weight)\n",
        "\n",
        "    jk                = (norm_factor*j_n_weight*k_n_weight)\n",
        "\n",
        "    if quaternion_format:\n",
        "        if scale is not None:\n",
        "            rot_kernel_1  = torch.cat([zero_kernel, scale * (1.0 - (square_j + square_k)), scale *(ij-rk), scale *(ik+rj)], dim=1)\n",
        "            rot_kernel_2  = torch.cat([zero_kernel, scale *(ij+rk), scale *(1.0 - (square_i + square_k)), scale *(jk-ri)], dim=1)\n",
        "            rot_kernel_3  = torch.cat([zero_kernel, scale *(ik-rj), scale *(jk+ri), scale *(1.0 - (square_i + square_j))], dim=1)\n",
        "        else:\n",
        "            rot_kernel_1  = torch.cat([zero_kernel, (1.0 - (square_j + square_k)), (ij-rk), (ik+rj)], dim=1)\n",
        "            rot_kernel_2  = torch.cat([zero_kernel, (ij+rk), (1.0 - (square_i + square_k)), (jk-ri)], dim=1)\n",
        "            rot_kernel_3  = torch.cat([zero_kernel, (ik-rj), (jk+ri), (1.0 - (square_i + square_j))], dim=1)\n",
        "\n",
        "        zero_kernel2  = torch.cat([zero_kernel, zero_kernel, zero_kernel, zero_kernel], dim=1)\n",
        "        global_rot_kernel = torch.cat([zero_kernel2, rot_kernel_1, rot_kernel_2, rot_kernel_3], dim=0)\n",
        "\n",
        "    else:\n",
        "        if scale is not None:\n",
        "            rot_kernel_1  = torch.cat([scale * (1.0 - (square_j + square_k)), scale *(ij-rk), scale *(ik+rj)], dim=0)\n",
        "            rot_kernel_2  = torch.cat([scale *(ij+rk), scale *(1.0 - (square_i + square_k)), scale *(jk-ri)], dim=0)\n",
        "            rot_kernel_3  = torch.cat([scale *(ik-rj), scale *(jk+ri), scale *(1.0 - (square_i + square_j))], dim=0)\n",
        "        else:\n",
        "            rot_kernel_1  = torch.cat([1.0 - (square_j + square_k), (ij-rk), (ik+rj)], dim=0)\n",
        "            rot_kernel_2  = torch.cat([(ij+rk), 1.0 - (square_i + square_k), (jk-ri)], dim=0)\n",
        "            rot_kernel_3  = torch.cat([(ik-rj), (jk+ri), (1.0 - (square_i + square_j))], dim=0)\n",
        "\n",
        "        global_rot_kernel = torch.cat([rot_kernel_1, rot_kernel_2, rot_kernel_3], dim=0)\n",
        "\n",
        "    #print(input.shape)\n",
        "    #print(square_r.shape)\n",
        "    #print(global_rot_kernel.shape)\n",
        "\n",
        "    if   input.dim() == 3:\n",
        "        convfunc = F.conv1d\n",
        "    elif input.dim() == 4:\n",
        "        convfunc = F.conv2d\n",
        "    elif input.dim() == 5:\n",
        "        convfunc = F.conv3d\n",
        "    else:\n",
        "        raise Exception(\"The convolutional input is either 3, 4 or 5 dimensions.\"\n",
        "                        \" input.dim = \" + str(input.dim()))\n",
        "\n",
        "    return convfunc(input, global_rot_kernel, bias, stride, padding, dilatation, groups)\n",
        "\n",
        "def quaternion_transpose_conv_rotation(input, zero_kernel, r_weight, i_weight, j_weight, k_weight, bias, stride,\n",
        "                    padding, output_padding, groups, dilatation, quaternion_format):\n",
        "    \"\"\"\n",
        "    Applies a quaternion rotation and transposed convolution transformation to the incoming data:\n",
        "    The rotation W*x*W^t can be replaced by R*x following:\n",
        "    https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n",
        "    Works for unitary and non unitary weights.\n",
        "    The initial size of the input must be a multiple of 3 if quaternion_format = False and\n",
        "    4 if quaternion_format = True.\n",
        "    \"\"\"\n",
        "\n",
        "    square_r          = (r_weight*r_weight)\n",
        "    square_i          = (i_weight*i_weight)\n",
        "    square_j          = (j_weight*j_weight)\n",
        "    square_k          = (k_weight*k_weight)\n",
        "\n",
        "    norm              = torch.sqrt(square_r+square_i+square_j+square_k + 0.0001)\n",
        "\n",
        "    r_weight          = (r_weight / norm)\n",
        "    i_weight          = (i_weight / norm)\n",
        "    j_weight          = (j_weight / norm)\n",
        "    k_weight          = (k_weight / norm)\n",
        "\n",
        "    norm_factor       = 2.0\n",
        "\n",
        "    square_i          = norm_factor*(i_weight*i_weight)\n",
        "    square_j          = norm_factor*(j_weight*j_weight)\n",
        "    square_k          = norm_factor*(k_weight*k_weight)\n",
        "\n",
        "    ri                = (norm_factor*r_weight*i_weight)\n",
        "    rj                = (norm_factor*r_weight*j_weight)\n",
        "    rk                = (norm_factor*r_weight*k_weight)\n",
        "\n",
        "    ij                = (norm_factor*i_weight*j_weight)\n",
        "    ik                = (norm_factor*i_weight*k_weight)\n",
        "\n",
        "    jk                = (norm_factor*j_weight*k_weight)\n",
        "\n",
        "    if quaternion_format:\n",
        "        rot_kernel_1  = torch.cat([zero_kernel, 1.0 - (square_j + square_k), ij-rk, ik+rj], dim=1)\n",
        "        rot_kernel_2  = torch.cat([zero_kernel, ij+rk, 1.0 - (square_i + square_k), jk-ri], dim=1)\n",
        "        rot_kernel_3  = torch.cat([zero_kernel, ik-rj, jk+ri, 1.0 - (square_i + square_j)], dim=1)\n",
        "\n",
        "        zero_kernel2  = torch.zeros(rot_kernel_1.shape).cuda()\n",
        "        global_rot_kernel = torch.cat([zero_kernel2, rot_kernel_1, rot_kernel_2, rot_kernel_3], dim=0)\n",
        "    else:\n",
        "        rot_kernel_1  = torch.cat([1.0 - (square_j + square_k), ij-rk, ik+rj], dim=1)\n",
        "        rot_kernel_2  = torch.cat([ij+rk, 1.0 - (square_i + square_k), jk-ri], dim=1)\n",
        "        rot_kernel_3  = torch.cat([ik-rj, jk+ri, 1.0 - (square_i + square_j)], dim=1)\n",
        "        global_rot_kernel = torch.cat([rot_kernel_1, rot_kernel_2, rot_kernel_3], dim=0)\n",
        "\n",
        "\n",
        "    if   input.dim() == 3:\n",
        "        convfunc = F.conv_transpose1d\n",
        "    elif input.dim() == 4:\n",
        "        convfunc = F.conv_transpose2d\n",
        "    elif input.dim() == 5:\n",
        "        convfunc = F.conv_transpose3d\n",
        "    else:\n",
        "        raise Exception(\"The convolutional input is either 3, 4 or 5 dimensions.\"\n",
        "                        \" input.dim = \" + str(input.dim()))\n",
        "\n",
        "    return convfunc(input, cat_kernels_4_quaternion, bias, stride, padding, output_padding, groups, dilatation)\n",
        "\n",
        "\n",
        "def quaternion_linear(input, r_weight, i_weight, j_weight, k_weight, bias=True):\n",
        "    \"\"\"\n",
        "    Applies a quaternion linear transformation to the incoming data:\n",
        "    It is important to notice that the forward phase of a QNN is defined\n",
        "    as W * Inputs (with * equal to the Hamilton product). The constructed\n",
        "    cat_kernels_4_quaternion is a modified version of the quaternion representation\n",
        "    so when we do torch.mm(Input,W) it's equivalent to W * Inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    cat_kernels_4_r = torch.cat([r_weight, -i_weight, -j_weight, -k_weight], dim=0)\n",
        "    cat_kernels_4_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=0)\n",
        "    cat_kernels_4_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=0)\n",
        "    cat_kernels_4_k = torch.cat([k_weight,  -j_weight, i_weight, r_weight], dim=0)\n",
        "    cat_kernels_4_quaternion   = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=1)\n",
        "\n",
        "    if input.dim() == 2 :\n",
        "\n",
        "        if bias is not None:\n",
        "            return torch.addmm(bias, input, cat_kernels_4_quaternion)\n",
        "        else:\n",
        "            return torch.mm(input, cat_kernels_4_quaternion)\n",
        "    else:\n",
        "        output = torch.matmul(input, cat_kernels_4_quaternion)\n",
        "        if bias is not None:\n",
        "            return output+bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "def quaternion_linear_rotation(input, zero_kernel, r_weight, i_weight, j_weight, k_weight, bias=None,\n",
        "                               quaternion_format=False, scale=None):\n",
        "    \"\"\"\n",
        "    Applies a quaternion rotation transformation to the incoming data:\n",
        "    The rotation W*x*W^t can be replaced by R*x following:\n",
        "    https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n",
        "    Works for unitary and non unitary weights.\n",
        "    The initial size of the input must be a multiple of 3 if quaternion_format = False and\n",
        "    4 if quaternion_format = True.\n",
        "    \"\"\"\n",
        "\n",
        "    square_r          = (r_weight*r_weight)\n",
        "    square_i          = (i_weight*i_weight)\n",
        "    square_j          = (j_weight*j_weight)\n",
        "    square_k          = (k_weight*k_weight)\n",
        "\n",
        "    norm              = torch.sqrt(square_r+square_i+square_j+square_k + 0.0001)\n",
        "\n",
        "    r_n_weight          = (r_weight / norm)\n",
        "    i_n_weight          = (i_weight / norm)\n",
        "    j_n_weight          = (j_weight / norm)\n",
        "    k_n_weight          = (k_weight / norm)\n",
        "\n",
        "    norm_factor       = 2.0\n",
        "\n",
        "    square_i          = norm_factor*(i_n_weight*i_n_weight)\n",
        "    square_j          = norm_factor*(j_n_weight*j_n_weight)\n",
        "    square_k          = norm_factor*(k_n_weight*k_n_weight)\n",
        "\n",
        "    ri                = (norm_factor*r_n_weight*i_n_weight)\n",
        "    rj                = (norm_factor*r_n_weight*j_n_weight)\n",
        "    rk                = (norm_factor*r_n_weight*k_n_weight)\n",
        "\n",
        "    ij                = (norm_factor*i_n_weight*j_n_weight)\n",
        "    ik                = (norm_factor*i_n_weight*k_n_weight)\n",
        "\n",
        "    jk                = (norm_factor*j_n_weight*k_n_weight)\n",
        "\n",
        "    if quaternion_format:\n",
        "        if scale is not None:\n",
        "            rot_kernel_1  = torch.cat([zero_kernel, scale * (1.0 - (square_j + square_k)), scale *(ij-rk), scale *(ik+rj)], dim=0)\n",
        "            rot_kernel_2  = torch.cat([zero_kernel, scale *(ij+rk), scale *(1.0 - (square_i + square_k)), scale *(jk-ri)], dim=0)\n",
        "            rot_kernel_3  = torch.cat([zero_kernel, scale *(ik-rj), scale *(jk+ri), scale *(1.0 - (square_i + square_j))], dim=0)\n",
        "        else:\n",
        "            rot_kernel_1  = torch.cat([zero_kernel, (1.0 - (square_j + square_k)), (ij-rk), (ik+rj)], dim=0)\n",
        "            rot_kernel_2  = torch.cat([zero_kernel, (ij+rk), (1.0 - (square_i + square_k)), (jk-ri)], dim=0)\n",
        "            rot_kernel_3  = torch.cat([zero_kernel, (ik-rj), (jk+ri), (1.0 - (square_i + square_j))], dim=0)\n",
        "\n",
        "        zero_kernel2  = torch.cat([zero_kernel, zero_kernel, zero_kernel, zero_kernel], dim=0)\n",
        "        global_rot_kernel = torch.cat([zero_kernel2, rot_kernel_1, rot_kernel_2, rot_kernel_3], dim=1)\n",
        "\n",
        "    else:\n",
        "        if scale is not None:\n",
        "            rot_kernel_1  = torch.cat([scale * (1.0 - (square_j + square_k)), scale *(ij-rk), scale *(ik+rj)], dim=0)\n",
        "            rot_kernel_2  = torch.cat([scale *(ij+rk), scale *(1.0 - (square_i + square_k)), scale *(jk-ri)], dim=0)\n",
        "            rot_kernel_3  = torch.cat([scale *(ik-rj), scale *(jk+ri), scale *(1.0 - (square_i + square_j))], dim=0)\n",
        "        else:\n",
        "            rot_kernel_1  = torch.cat([1.0 - (square_j + square_k), (ij-rk), (ik+rj)], dim=0)\n",
        "            rot_kernel_2  = torch.cat([(ij+rk), 1.0 - (square_i + square_k), (jk-ri)], dim=0)\n",
        "            rot_kernel_3  = torch.cat([(ik-rj), (jk+ri), (1.0 - (square_i + square_j))], dim=0)\n",
        "\n",
        "        global_rot_kernel = torch.cat([rot_kernel_1, rot_kernel_2, rot_kernel_3], dim=1)\n",
        "\n",
        "\n",
        "    if input.dim() == 2 :\n",
        "        if bias is not None:\n",
        "            return torch.addmm(bias, input, global_rot_kernel)\n",
        "        else:\n",
        "            return torch.mm(input, global_rot_kernel)\n",
        "    else:\n",
        "        output = torch.matmul(input, global_rot_kernel)\n",
        "        if bias is not None:\n",
        "            return output+bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "# Custom AUTOGRAD for lower VRAM consumption\n",
        "class QuaternionLinearFunction(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, r_weight, i_weight, j_weight, k_weight, bias=None):\n",
        "        ctx.save_for_backward(input, r_weight, i_weight, j_weight, k_weight, bias)\n",
        "        check_input(input)\n",
        "        cat_kernels_4_r = torch.cat([r_weight, -i_weight, -j_weight, -k_weight], dim=0)\n",
        "        cat_kernels_4_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=0)\n",
        "        cat_kernels_4_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=0)\n",
        "        cat_kernels_4_k = torch.cat([k_weight,  -j_weight, i_weight, r_weight], dim=0)\n",
        "        cat_kernels_4_quaternion = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=1)\n",
        "        if input.dim() == 2 :\n",
        "            if bias is not None:\n",
        "                return torch.addmm(bias, input, cat_kernels_4_quaternion)\n",
        "            else:\n",
        "                return torch.mm(input, cat_kernels_4_quaternion)\n",
        "        else:\n",
        "            output = torch.matmul(input, cat_kernels_4_quaternion)\n",
        "            if bias is not None:\n",
        "                return output+bias\n",
        "            else:\n",
        "                return output\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "\n",
        "        input, r_weight, i_weight, j_weight, k_weight, bias = ctx.saved_tensors\n",
        "        grad_input = grad_weight_r = grad_weight_i = grad_weight_j = grad_weight_k = grad_bias = None\n",
        "\n",
        "        input_r = torch.cat([r_weight, -i_weight, -j_weight, -k_weight], dim=0)\n",
        "        input_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=0)\n",
        "        input_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=0)\n",
        "        input_k = torch.cat([k_weight,  -j_weight, i_weight, r_weight], dim=0)\n",
        "        cat_kernels_4_quaternion_T = Variable(torch.cat([input_r, input_i, input_j, input_k], dim=1).permute(1,0), requires_grad=False)\n",
        "\n",
        "        r = get_r(input)\n",
        "        i = get_i(input)\n",
        "        j = get_j(input)\n",
        "        k = get_k(input)\n",
        "        input_r = torch.cat([r, -i, -j, -k], dim=0)\n",
        "        input_i = torch.cat([i,  r, -k, j], dim=0)\n",
        "        input_j = torch.cat([j,  k, r, -i], dim=0)\n",
        "        input_k = torch.cat([k,  -j, i, r], dim=0)\n",
        "        input_mat = Variable(torch.cat([input_r, input_i, input_j, input_k], dim=1), requires_grad=False)\n",
        "\n",
        "        r = get_r(grad_output)\n",
        "        i = get_i(grad_output)\n",
        "        j = get_j(grad_output)\n",
        "        k = get_k(grad_output)\n",
        "        input_r = torch.cat([r, i, j, k], dim=1)\n",
        "        input_i = torch.cat([-i,  r, k, -j], dim=1)\n",
        "        input_j = torch.cat([-j,  -k, r, i], dim=1)\n",
        "        input_k = torch.cat([-k,  j, -i, r], dim=1)\n",
        "        grad_mat = torch.cat([input_r, input_i, input_j, input_k], dim=0)\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input  = grad_output.mm(cat_kernels_4_quaternion_T)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_mat.permute(1,0).mm(input_mat).permute(1,0)\n",
        "            unit_size_x = r_weight.size(0)\n",
        "            unit_size_y = r_weight.size(1)\n",
        "            grad_weight_r = grad_weight.narrow(0,0,unit_size_x).narrow(1,0,unit_size_y)\n",
        "            grad_weight_i = grad_weight.narrow(0,0,unit_size_x).narrow(1,unit_size_y,unit_size_y)\n",
        "            grad_weight_j = grad_weight.narrow(0,0,unit_size_x).narrow(1,unit_size_y*2,unit_size_y)\n",
        "            grad_weight_k = grad_weight.narrow(0,0,unit_size_x).narrow(1,unit_size_y*3,unit_size_y)\n",
        "        if ctx.needs_input_grad[5]:\n",
        "            grad_bias   = grad_output.sum(0).squeeze(0)\n",
        "\n",
        "        return grad_input, grad_weight_r, grad_weight_i, grad_weight_j, grad_weight_k, grad_bias\n",
        "\n",
        "def hamilton_product(q0, q1):\n",
        "    \"\"\"\n",
        "    Applies a Hamilton product q0 * q1:\n",
        "    Shape:\n",
        "        - q0, q1 should be (batch_size, quaternion_number)\n",
        "        (rr' - xx' - yy' - zz')  +\n",
        "        (rx' + xr' + yz' - zy')i +\n",
        "        (ry' - xz' + yr' + zx')j +\n",
        "        (rz' + xy' - yx' + zr')k +\n",
        "    \"\"\"\n",
        "\n",
        "    q1_r = get_r(q1)\n",
        "    q1_i = get_i(q1)\n",
        "    q1_j = get_j(q1)\n",
        "    q1_k = get_k(q1)\n",
        "\n",
        "    # rr', xx', yy', and zz'\n",
        "    r_base = torch.mul(q0, q1)\n",
        "    # (rr' - xx' - yy' - zz')\n",
        "    r   = get_r(r_base) - get_i(r_base) - get_j(r_base) - get_k(r_base)\n",
        "\n",
        "    # rx', xr', yz', and zy'\n",
        "    i_base = torch.mul(q0, torch.cat([q1_i, q1_r, q1_k, q1_j], dim=1))\n",
        "    # (rx' + xr' + yz' - zy')\n",
        "    i   = get_r(i_base) + get_i(i_base) + get_j(i_base) - get_k(i_base)\n",
        "\n",
        "    # ry', xz', yr', and zx'\n",
        "    j_base = torch.mul(q0, torch.cat([q1_j, q1_k, q1_r, q1_i], dim=1))\n",
        "    # (rx' + xr' + yz' - zy')\n",
        "    j   = get_r(j_base) - get_i(j_base) + get_j(j_base) + get_k(j_base)\n",
        "\n",
        "    # rz', xy', yx', and zr'\n",
        "    k_base = torch.mul(q0, torch.cat([q1_k, q1_j, q1_i, q1_r], dim=1))\n",
        "    # (rx' + xr' + yz' - zy')\n",
        "    k   = get_r(k_base) + get_i(k_base) - get_j(k_base) + get_k(k_base)\n",
        "\n",
        "    return torch.cat([r, i, j, k], dim=1)\n",
        "\n",
        "#\n",
        "# PARAMETERS INITIALIZATION\n",
        "#\n",
        "\n",
        "def unitary_init(in_features, out_features, rng, kernel_size=None, criterion='he'):\n",
        "\n",
        "    if kernel_size is not None:\n",
        "        receptive_field = np.prod(kernel_size)\n",
        "        fan_in          = in_features  * receptive_field\n",
        "        fan_out         = out_features * receptive_field\n",
        "    else:\n",
        "        fan_in          = in_features\n",
        "        fan_out         = out_features\n",
        "\n",
        "\n",
        "    if kernel_size is None:\n",
        "        kernel_shape = (in_features, out_features)\n",
        "    else:\n",
        "        if type(kernel_size) is int:\n",
        "            kernel_shape = (out_features, in_features) + tuple((kernel_size,))\n",
        "        else:\n",
        "            kernel_shape = (out_features, in_features) + (*kernel_size,)\n",
        "\n",
        "    number_of_weights = np.prod(kernel_shape)\n",
        "    v_r = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "    v_i = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "    v_j = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "    v_k = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "\n",
        "    # Unitary quaternion\n",
        "    for i in range(0, number_of_weights):\n",
        "        norm = np.sqrt(v_r[i]**2 + v_i[i]**2 + v_j[i]**2 + v_k[i]**2)+0.0001\n",
        "        v_r[i]/= norm\n",
        "        v_i[i]/= norm\n",
        "        v_j[i]/= norm\n",
        "        v_k[i]/= norm\n",
        "    v_r = v_r.reshape(kernel_shape)\n",
        "    v_i = v_i.reshape(kernel_shape)\n",
        "    v_j = v_j.reshape(kernel_shape)\n",
        "    v_k = v_k.reshape(kernel_shape)\n",
        "\n",
        "    return (v_r, v_i, v_j, v_k)\n",
        "\n",
        "def random_init(in_features, out_features, rng, kernel_size=None, criterion='glorot'):\n",
        "\n",
        "    if kernel_size is not None:\n",
        "        receptive_field = np.prod(kernel_size)\n",
        "        fan_in          = in_features  * receptive_field\n",
        "        fan_out         = out_features * receptive_field\n",
        "    else:\n",
        "        fan_in          = in_features\n",
        "        fan_out         = out_features\n",
        "\n",
        "    if criterion == 'glorot':\n",
        "        s = 1. / np.sqrt(2*(fan_in + fan_out))\n",
        "    elif criterion == 'he':\n",
        "        s = 1. / np.sqrt(2*fan_in)\n",
        "    else:\n",
        "        raise ValueError('Invalid criterion: ' + criterion)\n",
        "\n",
        "    if kernel_size is None:\n",
        "        kernel_shape = (in_features, out_features)\n",
        "    else:\n",
        "        if type(kernel_size) is int:\n",
        "            kernel_shape = (out_features, in_features) + tuple((kernel_size,))\n",
        "        else:\n",
        "            kernel_shape = (out_features, in_features) + (*kernel_size,)\n",
        "\n",
        "    number_of_weights = np.prod(kernel_shape)\n",
        "    v_r = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "    v_i = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "    v_j = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "    v_k = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "\n",
        "\n",
        "\n",
        "    v_r = v_r.reshape(kernel_shape)\n",
        "    v_i = v_i.reshape(kernel_shape)\n",
        "    v_j = v_j.reshape(kernel_shape)\n",
        "    v_k = v_k.reshape(kernel_shape)\n",
        "\n",
        "    weight_r = v_r\n",
        "    weight_i = v_i\n",
        "    weight_j = v_j\n",
        "    weight_k = v_k\n",
        "    return (weight_r, weight_i, weight_j, weight_k)\n",
        "\n",
        "\n",
        "def quaternion_init(in_features, out_features, rng, kernel_size=None, criterion='glorot'):\n",
        "\n",
        "    if kernel_size is not None:\n",
        "        receptive_field = np.prod(kernel_size)\n",
        "        fan_in          = in_features  * receptive_field\n",
        "        fan_out         = out_features * receptive_field\n",
        "    else:\n",
        "        fan_in          = in_features\n",
        "        fan_out         = out_features\n",
        "\n",
        "    if criterion == 'glorot':\n",
        "        s = 1. / np.sqrt(2*(fan_in + fan_out))\n",
        "    elif criterion == 'he':\n",
        "        s = 1. / np.sqrt(2*fan_in)\n",
        "    else:\n",
        "        raise ValueError('Invalid criterion: ' + criterion)\n",
        "\n",
        "    rng = RandomState(np.random.randint(1,1234))\n",
        "\n",
        "    # Generating randoms and purely imaginary quaternions :\n",
        "    if kernel_size is None:\n",
        "        kernel_shape = (in_features, out_features)\n",
        "    else:\n",
        "        if type(kernel_size) is int:\n",
        "            kernel_shape = (out_features, in_features) + tuple((kernel_size,))\n",
        "        else:\n",
        "            kernel_shape = (out_features, in_features) + (*kernel_size,)\n",
        "\n",
        "    modulus = chi.rvs(4,loc=0,scale=s,size=kernel_shape)\n",
        "    number_of_weights = np.prod(kernel_shape)\n",
        "    v_i = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "    v_j = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "    v_k = np.random.uniform(-1.0,1.0,number_of_weights)\n",
        "\n",
        "    # Purely imaginary quaternions unitary\n",
        "    for i in range(0, number_of_weights):\n",
        "        norm = np.sqrt(v_i[i]**2 + v_j[i]**2 + v_k[i]**2 +0.0001)\n",
        "        v_i[i]/= norm\n",
        "        v_j[i]/= norm\n",
        "        v_k[i]/= norm\n",
        "    v_i = v_i.reshape(kernel_shape)\n",
        "    v_j = v_j.reshape(kernel_shape)\n",
        "    v_k = v_k.reshape(kernel_shape)\n",
        "\n",
        "    phase = rng.uniform(low=-np.pi, high=np.pi, size=kernel_shape)\n",
        "\n",
        "    weight_r = modulus * np.cos(phase)\n",
        "    weight_i = modulus * v_i*np.sin(phase)\n",
        "    weight_j = modulus * v_j*np.sin(phase)\n",
        "    weight_k = modulus * v_k*np.sin(phase)\n",
        "\n",
        "    return (weight_r, weight_i, weight_j, weight_k)\n",
        "\n",
        "def create_dropout_mask(dropout_p, size, rng, as_type, operation='linear'):\n",
        "    if operation == 'linear':\n",
        "        mask = rng.binomial(n=1, p=1-dropout_p, size=size)\n",
        "        return Variable(torch.from_numpy(mask).type(as_type))\n",
        "    else:\n",
        "         raise Exception(\"create_dropout_mask accepts only 'linear'. Found operation = \"\n",
        "                        + str(operation))\n",
        "\n",
        "def affect_init(r_weight, i_weight, j_weight, k_weight, init_func, rng, init_criterion):\n",
        "    if r_weight.size() != i_weight.size() or r_weight.size() != j_weight.size() or \\\n",
        "    r_weight.size() != k_weight.size() :\n",
        "         raise ValueError('The real and imaginary weights '\n",
        "                 'should have the same size . Found: r:'\n",
        "                 + str(r_weight.size()) +' i:'\n",
        "                 + str(i_weight.size()) +' j:'\n",
        "                 + str(j_weight.size()) +' k:'\n",
        "                 + str(k_weight.size()))\n",
        "\n",
        "    elif r_weight.dim() != 2:\n",
        "        raise Exception('affect_init accepts only matrices. Found dimension = '\n",
        "                        + str(r_weight.dim()))\n",
        "    kernel_size = None\n",
        "    r, i, j, k  = init_func(r_weight.size(0), r_weight.size(1), rng, kernel_size, init_criterion)\n",
        "    r, i, j, k  = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
        "    r_weight.data = r.type_as(r_weight.data)\n",
        "    i_weight.data = i.type_as(i_weight.data)\n",
        "    j_weight.data = j.type_as(j_weight.data)\n",
        "    k_weight.data = k.type_as(k_weight.data)\n",
        "\n",
        "\n",
        "def affect_init_conv(r_weight, i_weight, j_weight, k_weight, kernel_size, init_func, rng,\n",
        "                     init_criterion):\n",
        "    if r_weight.size() != i_weight.size() or r_weight.size() != j_weight.size() or \\\n",
        "    r_weight.size() != k_weight.size() :\n",
        "         raise ValueError('The real and imaginary weights '\n",
        "                 'should have the same size . Found: r:'\n",
        "                 + str(r_weight.size()) +' i:'\n",
        "                 + str(i_weight.size()) +' j:'\n",
        "                 + str(j_weight.size()) +' k:'\n",
        "                 + str(k_weight.size()))\n",
        "\n",
        "    elif 2 >= r_weight.dim():\n",
        "        raise Exception('affect_conv_init accepts only tensors that have more than 2 dimensions. Found dimension = '\n",
        "                        + str(real_weight.dim()))\n",
        "\n",
        "    r, i, j, k = init_func(\n",
        "        r_weight.size(1),\n",
        "        r_weight.size(0),\n",
        "        rng=rng,\n",
        "        kernel_size=kernel_size,\n",
        "        criterion=init_criterion\n",
        "    )\n",
        "    r, i, j, k = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n",
        "    r_weight.data = r.type_as(r_weight.data)\n",
        "    i_weight.data = i.type_as(i_weight.data)\n",
        "    j_weight.data = j.type_as(j_weight.data)\n",
        "    k_weight.data = k.type_as(k_weight.data)\n",
        "\n",
        "def get_kernel_and_weight_shape(operation, in_channels, out_channels, kernel_size):\n",
        "    if operation == 'convolution1d':\n",
        "        if type(kernel_size) is not int:\n",
        "            raise ValueError(\n",
        "                \"\"\"An invalid kernel_size was supplied for a 1d convolution. The kernel size\n",
        "                must be integer in the case. Found kernel_size = \"\"\" + str(kernel_size)\n",
        "            )\n",
        "        else:\n",
        "            ks = kernel_size\n",
        "            w_shape = (out_channels, in_channels) + tuple((ks,))\n",
        "    else:# in case it is 2d or 3d.\n",
        "        if   operation == 'convolution2d' and type(kernel_size) is int:\n",
        "            ks = (kernel_size, kernel_size)\n",
        "        elif operation == 'convolution3d' and type(kernel_size) is int:\n",
        "            ks = (kernel_size, kernel_size, kernel_size)\n",
        "        elif type(kernel_size) is not int:\n",
        "            if   operation == 'convolution2d' and len(kernel_size) != 2:\n",
        "                raise ValueError(\n",
        "                    \"\"\"An invalid kernel_size was supplied for a 2d convolution. The kernel size\n",
        "                    must be either an integer or a tuple of 2. Found kernel_size = \"\"\" + str(kernel_size)\n",
        "                )\n",
        "            elif operation == 'convolution3d' and len(kernel_size) != 3:\n",
        "                raise ValueError(\n",
        "                    \"\"\"An invalid kernel_size was supplied for a 3d convolution. The kernel size\n",
        "                    must be either an integer or a tuple of 3. Found kernel_size = \"\"\" + str(kernel_size)\n",
        "                )\n",
        "            else:\n",
        "                ks = kernel_size\n",
        "        w_shape = (out_channels, in_channels) + (*ks,)\n",
        "    return ks, w_shape"
      ],
      "metadata": {
        "id": "sjNMdWdnfryn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the quaternion layers\n",
        "core_qnn/quaternion_layers.py\n"
      ],
      "metadata": {
        "id": "ST8EI9T-dRme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuaternionTransposeConv(Module):\n",
        "    r\"\"\"Applies a Quaternion Transposed Convolution (or Deconvolution) to the incoming data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 dilatation=1, padding=0, output_padding=0, groups=1, bias=True, init_criterion='he',\n",
        "                 weight_init='quaternion', seed=None, operation='convolution2d', rotation=False,\n",
        "                 quaternion_format=False):\n",
        "\n",
        "        super(QuaternionTransposeConv, self).__init__()\n",
        "\n",
        "        self.in_channels       = in_channels  // 4\n",
        "        self.out_channels      = out_channels // 4\n",
        "        self.stride            = stride\n",
        "        self.padding           = padding\n",
        "        self.output_padding    = output_padding\n",
        "        self.groups            = groups\n",
        "        self.dilatation        = dilatation\n",
        "        self.init_criterion    = init_criterion\n",
        "        self.weight_init       = weight_init\n",
        "        self.seed              = seed if seed is not None else np.random.randint(0,1234)\n",
        "        self.rng               = RandomState(self.seed)\n",
        "        self.operation         = operation\n",
        "        self.rotation          = rotation\n",
        "        self.quaternion_format = quaternion_format\n",
        "        self.winit             = {'quaternion': quaternion_init,\n",
        "                                  'unitary'   : unitary_init,\n",
        "                                  'random'    : random_init}[self.weight_init]\n",
        "\n",
        "\n",
        "        (self.kernel_size, self.w_shape) = get_kernel_and_weight_shape( self.operation,\n",
        "            self.out_channels, self.in_channels, kernel_size )\n",
        "\n",
        "        self.r_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.i_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.j_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.k_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        affect_init_conv(self.r_weight, self.i_weight, self.j_weight, self.k_weight,\n",
        "                    self.kernel_size, self.winit, self.rng, self.init_criterion)\n",
        "        if self.bias is not None:\n",
        "           self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        if self.rotation:\n",
        "            return quaternion_tranpose_conv_rotation(input, self.r_weight, self.i_weight,\n",
        "                self.j_weight, self.k_weight, self.bias, self.stride, self.padding,\n",
        "                self.output_padding, self.groups, self.dilatation, self.quaternion_format)\n",
        "        else:\n",
        "            return quaternion_transpose_conv(input, self.r_weight, self.i_weight, self.j_weight,\n",
        "                self.k_weight, self.bias, self.stride, self.padding, self.output_padding,\n",
        "                self.groups, self.dilatation)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_channels='      + str(self.in_channels) \\\n",
        "            + ', out_channels='   + str(self.out_channels) \\\n",
        "            + ', bias='           + str(self.bias is not None) \\\n",
        "            + ', kernel_size='    + str(self.kernel_size) \\\n",
        "            + ', stride='         + str(self.stride) \\\n",
        "            + ', padding='        + str(self.padding) \\\n",
        "            + ', dilation='       + str(self.dilation) \\\n",
        "            + ', init_criterion=' + str(self.init_criterion) \\\n",
        "            + ', weight_init='    + str(self.weight_init) \\\n",
        "            + ', seed='           + str(self.seed) \\\n",
        "            + ', operation='      + str(self.operation) + ')'"
      ],
      "metadata": {
        "id": "DEloN1wpdFm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuaternionConv(Module):\n",
        "    r\"\"\"Applies a Quaternion Convolution to the incoming data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 dilatation=1, padding=0, groups=1, bias=True, init_criterion='glorot',\n",
        "                 weight_init='quaternion', seed=None, operation='convolution2d', rotation=False, quaternion_format=True, scale=False):\n",
        "\n",
        "        super(QuaternionConv, self).__init__()\n",
        "\n",
        "        self.in_channels       = in_channels  // 4\n",
        "        self.out_channels      = out_channels // 4\n",
        "        self.stride            = stride\n",
        "        self.padding           = padding\n",
        "        self.groups            = groups\n",
        "        self.dilatation        = dilatation\n",
        "        self.init_criterion    = init_criterion\n",
        "        self.weight_init       = weight_init\n",
        "        self.seed              = seed if seed is not None else np.random.randint(0,1234)\n",
        "        self.rng               = RandomState(self.seed)\n",
        "        self.operation         = operation\n",
        "        self.rotation          = rotation\n",
        "        self.quaternion_format = quaternion_format\n",
        "        self.winit             =    {'quaternion': quaternion_init,\n",
        "                                     'unitary'   : unitary_init,\n",
        "                                     'random'    : random_init}[self.weight_init]\n",
        "        self.scale             = scale\n",
        "\n",
        "\n",
        "        (self.kernel_size, self.w_shape) = get_kernel_and_weight_shape( self.operation,\n",
        "            self.in_channels, self.out_channels, kernel_size )\n",
        "\n",
        "        self.r_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.i_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.j_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.k_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "\n",
        "        if self.scale:\n",
        "            self.scale_param  = Parameter(torch.Tensor(self.r_weight.shape))\n",
        "        else:\n",
        "            self.scale_param  = None\n",
        "\n",
        "        if self.rotation:\n",
        "            self.zero_kernel = Parameter(torch.zeros(self.r_weight.shape), requires_grad=False)\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        affect_init_conv(self.r_weight, self.i_weight, self.j_weight, self.k_weight,\n",
        "                    self.kernel_size, self.winit, self.rng, self.init_criterion)\n",
        "        if self.scale_param is not None:\n",
        "            torch.nn.init.xavier_uniform_(self.scale_param.data)\n",
        "        if self.bias is not None:\n",
        "           self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "\n",
        "        if self.rotation:\n",
        "            return quaternion_conv_rotation(input, self.zero_kernel, self.r_weight, self.i_weight, self.j_weight,\n",
        "                self.k_weight, self.bias, self.stride, self.padding, self.groups, self.dilatation,\n",
        "                self.quaternion_format, self.scale_param)\n",
        "        else:\n",
        "            return quaternion_conv(input, self.r_weight, self.i_weight, self.j_weight,\n",
        "                self.k_weight, self.bias, self.stride, self.padding, self.groups, self.dilatation)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_channels='      + str(self.in_channels) \\\n",
        "            + ', out_channels='   + str(self.out_channels) \\\n",
        "            + ', bias='           + str(self.bias is not None) \\\n",
        "            + ', kernel_size='    + str(self.kernel_size) \\\n",
        "            + ', stride='         + str(self.stride) \\\n",
        "            + ', padding='        + str(self.padding) \\\n",
        "            + ', init_criterion=' + str(self.init_criterion) \\\n",
        "            + ', weight_init='    + str(self.weight_init) \\\n",
        "            + ', seed='           + str(self.seed) \\\n",
        "            + ', rotation='       + str(self.rotation) \\\n",
        "            + ', q_format='       + str(self.quaternion_format) \\\n",
        "            + ', operation='      + str(self.operation) + ')'"
      ],
      "metadata": {
        "id": "vig8w0BgdJuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuaternionLinearAutograd(Module):\n",
        "    r\"\"\"Applies a quaternion linear transformation to the incoming data. A custom\n",
        "    Autograd function is call to drastically reduce the VRAM consumption. Nonetheless, computing\n",
        "    time is also slower compared to QuaternionLinear().\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 init_criterion='glorot', weight_init='quaternion',\n",
        "                 seed=None, rotation=False, quaternion_format=True, scale=False):\n",
        "\n",
        "        super(QuaternionLinearAutograd, self).__init__()\n",
        "        self.in_features       = in_features//4\n",
        "        self.out_features      = out_features//4\n",
        "        self.rotation          = rotation\n",
        "        self.quaternion_format = quaternion_format\n",
        "        self.r_weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.i_weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.j_weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.k_weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.scale    = scale\n",
        "\n",
        "        if self.scale:\n",
        "            self.scale_param  = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        else:\n",
        "            self.scale_param  = None\n",
        "\n",
        "        if self.rotation:\n",
        "            self.zero_kernel  = Parameter(torch.zeros(self.r_weight.shape), requires_grad=False)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(self.out_features*4))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.init_criterion = init_criterion\n",
        "        self.weight_init = weight_init\n",
        "        self.seed = seed if seed is not None else np.random.randint(0,1234)\n",
        "        self.rng = RandomState(self.seed)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        winit = {'quaternion': quaternion_init, 'unitary': unitary_init, 'random': random_init}[self.weight_init]\n",
        "        if self.scale_param is not None:\n",
        "            torch.nn.init.xavier_uniform_(self.scale_param.data)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.fill_(0)\n",
        "        affect_init(self.r_weight, self.i_weight, self.j_weight, self.k_weight, winit,\n",
        "                    self.rng, self.init_criterion)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # See the autograd section for explanation of what happens here.\n",
        "        if self.rotation:\n",
        "            return quaternion_linear_rotation(input, self.zero_kernel, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias, self.quaternion_format, self.scale_param)\n",
        "        else:\n",
        "            return quaternion_linear(input, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_features=' + str(self.in_features) \\\n",
        "            + ', out_features=' + str(self.out_features) \\\n",
        "            + ', bias=' + str(self.bias is not None) \\\n",
        "            + ', init_criterion=' + str(self.init_criterion) \\\n",
        "            + ', weight_init=' + str(self.weight_init) \\\n",
        "            + ', rotation='       + str(self.rotation) \\\n",
        "            + ', seed=' + str(self.seed) + ')'"
      ],
      "metadata": {
        "id": "EpTeIdM2dNiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuaternionLinear(Module):\n",
        "    r\"\"\"Applies a quaternion linear transformation to the incoming data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 init_criterion='he', weight_init='quaternion',\n",
        "                 seed=None):\n",
        "\n",
        "        super(QuaternionLinear, self).__init__()\n",
        "        self.in_features  = in_features//4\n",
        "        self.out_features = out_features//4\n",
        "        self.r_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.i_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.j_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.k_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "\n",
        "        if bias:\n",
        "            self.bias     = Parameter(torch.Tensor(self.out_features*4))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.init_criterion = init_criterion\n",
        "        self.weight_init    = weight_init\n",
        "        self.seed           = seed if seed is not None else np.random.randint(0,1234)\n",
        "        self.rng            = RandomState(self.seed)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        winit = {'quaternion': quaternion_init,\n",
        "                 'unitary': unitary_init}[self.weight_init]\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.fill_(0)\n",
        "        affect_init(self.r_weight, self.i_weight, self.j_weight, self.k_weight, winit,\n",
        "                    self.rng, self.init_criterion)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # See the autograd section for explanation of what happens here.\n",
        "        if input.dim() == 3:\n",
        "            T, N, C = input.size()\n",
        "            input  = input.view(T * N, C)\n",
        "            output = QuaternionLinearFunction.apply(input, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias)\n",
        "            output = output.view(T, N, output.size(1))\n",
        "        elif input.dim() == 2:\n",
        "            output = QuaternionLinearFunction.apply(input, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_features=' + str(self.in_features) \\\n",
        "            + ', out_features=' + str(self.out_features) \\\n",
        "            + ', bias=' + str(self.bias is not None) \\\n",
        "            + ', init_criterion=' + str(self.init_criterion) \\\n",
        "            + ', weight_init=' + str(self.weight_init) \\\n",
        "            + ', seed=' + str(self.seed) + ')'"
      ],
      "metadata": {
        "id": "E8D6OEFGdPnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reimplementation\n",
        "## Import required packages"
      ],
      "metadata": {
        "id": "47cABIb1orZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages to import Cifar\n",
        "from torchvision.datasets import CIFAR10\n",
        "# from torchvision.transforms import ToTensor\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "JfQ78J9ZrG3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import CIFAR"
      ],
      "metadata": {
        "id": "nZLGsmNS6RtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Cifar10\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def convert_to_quaternion(batch):\n",
        "  new_batch = torch.empty(len(batch), 4, 32, 32)\n",
        "  labels = torch.LongTensor(len(batch))\n",
        "\n",
        "  # Transform images to quaternion matrices\n",
        "  for i in range(len(batch)):\n",
        "    image, label = batch[i][0], batch[i][1]\n",
        "    real = torch.zeros(32, 32)\n",
        "    new_image = torch.cat((image, real.unsqueeze(0)), dim=0)\n",
        "    new_batch[i, :, :, :] = new_image\n",
        "    labels[i] = label\n",
        "\n",
        "  return new_batch, labels\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomAffine(degrees = 0, translate = (0.1, 0.1)),\n",
        "    transforms.RandomHorizontalFlip(0.5)\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='data/', download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='data/', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "torch.manual_seed(43)\n",
        "val_size = 0\n",
        "train_size = len(train_dataset) - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "##############################################################################\n",
        "# These values are directly copied from the Keras tensorflow implementation. #\n",
        "##############################################################################\n",
        "batch_size = 32\n",
        "\n",
        "num_predictions = 20\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model_Q_REIMPLEMENTATION.h5'\n",
        "\n",
        "# TODO:\n",
        "# - they split the data into categorical data. Do we have to do this too?\n",
        "# - Transfor data in pytorch, equaivilant of the data_agumentation in Tensorflow.\n",
        "\n",
        "##############################################################################\n",
        "# END END                                                                    #\n",
        "##############################################################################\n",
        "\n",
        "quaternion = False\n",
        "if quaternion:\n",
        "  train_loader = DataLoader(train_dataset, batch_size, shuffle=True, \n",
        "                          collate_fn=convert_to_quaternion)\n",
        "  test_loader = DataLoader(test_dataset, batch_size*2, num_workers=2, \n",
        "                         pin_memory=True, collate_fn=convert_to_quaternion)\n",
        "else:\n",
        "  train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size*2, num_workers=2, \n",
        "                         pin_memory=True)\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)"
      ],
      "metadata": {
        "id": "WOyuqd8VompS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "GpY_B-OK3xXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def try_gpu():\n",
        "    \"\"\"\n",
        "    If GPU is available, return torch.device as cuda:0; else return torch.device\n",
        "    as cpu.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      print(\"Training on GPU\")\n",
        "      device = torch.device('cuda:0')\n",
        "    else:\n",
        "      print(\"Training on CPU\")\n",
        "      device = torch.device('cpu')\n",
        "    return device"
      ],
      "metadata": {
        "id": "Cnu54RHkFUae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement baseline \n",
        "referred in the paper as \"real-valued CNN\""
      ],
      "metadata": {
        "id": "3Qwh2dFIrONV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    3-layer CNN network with max pooling\n",
        "    \n",
        "    Args:\n",
        "        in_channels: number of features of the input image (\"depth of image\")\n",
        "        hidden_channels: number of hidden features (\"depth of convolved images\")\n",
        "        out_features: number of features in output layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels: int, hidden_channels, out_features: int, kernel_size: int):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "          # block 1\n",
        "          nn.Conv2d(in_channels, hidden_channels[0], kernel_size, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_channels[0], hidden_channels[1], kernel_size),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, stride=2),\n",
        "          torch.nn.Dropout(p=0.25),\n",
        "\n",
        "          # block 2\n",
        "          nn.Conv2d(hidden_channels[1], hidden_channels[2], kernel_size, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_channels[2], hidden_channels[3], kernel_size),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, stride=2),\n",
        "          torch.nn.Dropout(p=0.25),\n",
        "\n",
        "          # block 3\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(2304, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.5),\n",
        "          nn.Linear(512, out_features),\n",
        "          nn.Softmax()\n",
        "        )\n",
        "    \n",
        "    def accuracy(self, outputs, labels):\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "      images, labels = batch\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      out = self(images)                  # Generate predictions\n",
        "      loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "      accu = self.accuracy(out,labels)\n",
        "\n",
        "      return loss, accu\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = self.accuracy(out, labels)           # Calculate accuracy\n",
        "\n",
        "        return loss.detach(), acc\n",
        "    \n",
        "    def epoch_end(self, epoch, train_acc, train_loss, test_acc, test_loss):\n",
        "        print(\"Epoch :\",epoch + 1)\n",
        "        print(f'Train Accuracy:{train_acc*100:.2f}% Validation Accuracy:{test_acc*100:.2f}%')\n",
        "        print(f'Train Loss:{train_loss:.4f} Validation Loss:{test_loss:.4f}')\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.net.forward(x)\n",
        "      \n"
      ],
      "metadata": {
        "id": "3dnAyWlOmJR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement QNN"
      ],
      "metadata": {
        "id": "A9YpyaJa9Shj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_features, kernel_size = 3):\n",
        "        super(QNN, self).__init__()\n",
        "    \n",
        "        self.net = nn.Sequential(\n",
        "            # Padding\n",
        "            QuaternionConv(in_channels, hidden_channels[0], \n",
        "                                     kernel_size, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            QuaternionConv(hidden_channels[0], hidden_channels[1],\n",
        "                              kernel_size, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            torch.nn.Dropout(p=0.25),\n",
        "\n",
        "            QuaternionConv(hidden_channels[1], hidden_channels[2],\n",
        "                              kernel_size, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            QuaternionConv(hidden_channels[2], hidden_channels[3],\n",
        "                              kernel_size, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            torch.nn.Dropout(p=0.25),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            # QDense\n",
        "            QuaternionLinear(2304, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            # Dense\n",
        "            nn.Linear(512, out_features),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "    \n",
        "    def accuracy(self, outputs, labels):\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "      images, labels = batch\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      out = self(images)                  # Generate predictions\n",
        "      loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "      accu = self.accuracy(out, labels)\n",
        "\n",
        "      return loss, accu\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = self.accuracy(out, labels)           # Calculate accuracy\n",
        "\n",
        "        return loss.detach(), acc\n",
        "    \n",
        "    def epoch_end(self, epoch, train_acc, train_loss, test_acc, test_loss):\n",
        "        print(\"Epoch :\",epoch + 1)\n",
        "        print(f'Train Accuracy:{train_acc*100:.2f}% Validation Accuracy:{test_acc*100:.2f}%')\n",
        "        print(f'Train Loss:{train_loss:.4f} Validation Loss:{test_loss:.4f}')\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.net.forward(x)"
      ],
      "metadata": {
        "id": "0LFih37j9Yin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "if quaternion:\n",
        "  in_channels = 4\n",
        "  hidden_channels = [32, 32, 64, 64]\n",
        "  out_features = 10\n",
        "\n",
        "  net = QNN(in_channels, hidden_channels, out_features)\n",
        "else:\n",
        "  in_channels = 3\n",
        "  hidden_channels = [32, 32, 64, 64]\n",
        "  out_features = 10\n",
        "\n",
        "  net = CNN(in_channels, hidden_channels, out_features, kernel_size=3)\n",
        "  # summary(net, (3, 32, 32))\n",
        "\n",
        "epochs = 80\n",
        "lr=0.0001\n",
        "decay=1e-6\n",
        "optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)"
      ],
      "metadata": {
        "id": "bVfEfc079b62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "93IZgCMq8avy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "running_loss = 0\n",
        "train_losses = []\n",
        "train_accuracy = []\n",
        "test_accs = []\n",
        "\n",
        "# learning rate decay\n",
        "# weight initialization\n",
        "\n",
        "device = try_gpu()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    batch_losses = []\n",
        "    batch_acc = []\n",
        "    for batch in data_loader:\n",
        "      loss, acc = model.validation_step(batch)\n",
        "      batch_losses.append(loss.item())\n",
        "      batch_acc.append(acc.item())\n",
        "    return mean(batch_losses), mean(batch_acc)\n",
        "\n",
        "best_valid = None\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch \" + str(epoch))\n",
        "\n",
        "    # Network in training mode and to device\n",
        "    net.train()\n",
        "    net.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for batch in tqdm(train_loader):\n",
        "        loss, accu = net.training_step(batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.cpu().item()\n",
        "\n",
        "    # Validation\n",
        "    test_loss, test_acc = evaluate(net, test_loader)\n",
        "    train_loss, train_acc = evaluate(net, train_loader)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    # Learning rate decay\n",
        "    # scheduler.step()\n",
        "\n",
        "    net.epoch_end(epoch, train_acc, train_loss, test_acc, test_loss)\n",
        "    if (best_valid == None or best_valid < test_acc):\n",
        "        best_valid = test_acc\n",
        "        torch.save(net.state_dict(), 'cifar10-cnn.pth')\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "x89xT1MF4wjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def plot_accuracies(train_accuracies, test_accuracies):\n",
        "    plt.plot(train_accuracies, '-rx')\n",
        "    plt.plot(test_accuracies, '-bx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Accuracy vs. No. of epochs')\n",
        "\n",
        "    if quaternion:\n",
        "      with open('qnn', 'w') as f:\n",
        "          # create the csv writer\n",
        "          writer = csv.writer(f)\n",
        "\n",
        "          # write a row to the csv file\n",
        "          writer.writerow(test_accuracies)\n",
        "          writer.writerow(train_losses)\n",
        "    else:\n",
        "      with open('cnn', 'w') as f:\n",
        "          # create the csv writer\n",
        "          writer = csv.writer(f)\n",
        "\n",
        "          # write a row to the csv file\n",
        "          writer.writerow(test_accuracies)\n",
        "          writer.writerow(train_losses)\n",
        "\n",
        "plot_accuracies(train_accuracies, test_accuracies)"
      ],
      "metadata": {
        "id": "l55gqXT3ZVD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results of the real-valued CNN"
      ],
      "metadata": {
        "id": "6Twh_ihzJQ0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(train_losses)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "CBt-mY1uojv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}